{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets pytorch-ignite\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -qq 'tiny-imagenet-200.zip'\n",
        "!wget -c https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xvzf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "id": "OWUN0igNALog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wAQK2iS_nhQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime as dt\n",
        "\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import models, datasets\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss, Precision, Recall\n",
        "from ignite.handlers import LRScheduler, ModelCheckpoint, global_step_from_engine\n",
        "from ignite.contrib.handlers import ProgressBar, TensorboardLogger\n",
        "import ignite.contrib.engines.common as common\n",
        "\n",
        "from torchvision.models import efficientnet_b3\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import opendatasets as od\n",
        "import os\n",
        "from random import randint\n",
        "import urllib\n",
        "import zipfile\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PBSywIN_nhR"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
        "# Define training and validation data paths\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
        "VALID_DIR = os.path.join(DATA_DIR, 'val')"
      ],
      "metadata": {
        "id": "2G6Js-FTZqbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQPhc0bI_nhS"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "    \n",
        "def show_batch(dataloader):\n",
        "    dataiter = iter(dataloader)\n",
        "    # images, labels = dataiter.next()\n",
        "    images, labels = next(dataiter)    \n",
        "    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
        "    \n",
        "def show_image(dataloader):\n",
        "    dataiter = iter(dataloader)\n",
        "    images, labels = dataiter.next()\n",
        "    random_num = randint(0, len(images)-1)\n",
        "    imshow(images[random_num])\n",
        "    label = labels[random_num]\n",
        "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
        "\n",
        "# Setup function to create dataloaders for image datasets\n",
        "def generate_dataloader(data, name, transform):\n",
        "    if data is None: \n",
        "        return None\n",
        "    \n",
        "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
        "    # loader where images are in format root/label/filename\n",
        "    # See https://pytorch.org/vision/stable/datasets.html\n",
        "    if transform is None:\n",
        "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
        "    else:\n",
        "        dataset = datasets.ImageFolder(data, transform=transform)\n",
        "\n",
        "    # Set options for device\n",
        "    kwargs = {}\n",
        "    \n",
        "    # Wrap image dataset (defined above) in dataloader \n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                        shuffle=(name==\"train\"), \n",
        "                        **kwargs)\n",
        "    \n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5a1Es7i_nhT"
      },
      "outputs": [],
      "source": [
        "val_data = pd.read_csv(f'{VALID_DIR}/val_annotations.txt', \n",
        "                       sep='\\t', \n",
        "                       header=None, \n",
        "                       names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "\n",
        "val_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roqw3GjD_nhT"
      },
      "outputs": [],
      "source": [
        "# Create separate validation subfolders for the validation images based on\n",
        "# their labels indicated in the val_annotations txt file\n",
        "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
        "\n",
        "# Open and read val annotations text file\n",
        "fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
        "data = fp.readlines()\n",
        "\n",
        "# Create dictionary to store img filename (word 0) and corresponding\n",
        "# label (word 1) for every line in the txt file (as key value pair)\n",
        "val_img_dict = {}\n",
        "for line in data:\n",
        "    words = line.split('\\t')\n",
        "    val_img_dict[words[0]] = words[1]\n",
        "fp.close()\n",
        "\n",
        "# Display first 10 entries of resulting val_img_dict dictionary\n",
        "{k: val_img_dict[k] for k in list(val_img_dict)[:10]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBsSlp73_nhU"
      },
      "outputs": [],
      "source": [
        "# Create subfolders (if not present) for validation images based on label ,\n",
        "# and move images into the respective folders\n",
        "for img, folder in val_img_dict.items():\n",
        "    newpath = (os.path.join(val_img_dir, folder))\n",
        "    if not os.path.exists(newpath):\n",
        "        os.makedirs(newpath)\n",
        "    if os.path.exists(os.path.join(val_img_dir, img)):\n",
        "        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYczGu52_nhU"
      },
      "outputs": [],
      "source": [
        "# Save class names (for corresponding labels) as dict from words.txt file\n",
        "class_to_name_dict = dict()\n",
        "fp = open(os.path.join(DATA_DIR, 'words.txt'), 'r')\n",
        "data = fp.readlines()\n",
        "for line in data:\n",
        "    words = line.strip('\\n').split('\\t')\n",
        "    class_to_name_dict[words[0]] = words[1].split(',')[0]\n",
        "fp.close()\n",
        "\n",
        "# Display first 20 entries of resulting dictionary\n",
        "{k: class_to_name_dict[k] for k in list(class_to_name_dict)[:20]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwhTxYw__nhV"
      },
      "outputs": [],
      "source": [
        "preprocess_transform = T.Compose([\n",
        "                T.Resize(256), # Resize images to 256 x 256\n",
        "                T.CenterCrop(224), # Center crop image\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.ToTensor(),  # Converting cropped images to tensors\n",
        "                # T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # \n",
        "])\n",
        "\n",
        "preprocess_transform_pretrain = T.Compose([\n",
        "                T.Resize(256), # Resize images to 256 x 256\n",
        "                T.CenterCrop(224), # Center crop image\n",
        "                T.RandomHorizontalFlip(),\n",
        "                T.ToTensor(),  # Converting cropped images to tensors\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                            std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc22Z7C7_nhV"
      },
      "outputs": [],
      "source": [
        "# Define batch size for data loaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = generate_dataloader(TRAIN_DIR, \"train\", transform = preprocess_transform)\n",
        "\n",
        "# Display batch of training set images\n",
        "show_batch(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7j4v4fB_nhV"
      },
      "outputs": [],
      "source": [
        "# Create train loader for pre-trained models (normalized based on specific requirements)\n",
        "train_loader_pretrain = generate_dataloader(TRAIN_DIR, \"train\", transform=preprocess_transform_pretrain)\n",
        "show_batch(train_loader_pretrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxrqRUzq_nhV"
      },
      "outputs": [],
      "source": [
        "val_loader = generate_dataloader(val_img_dir, \"val\", transform=preprocess_transform)\n",
        "\n",
        "val_loader_pretrain = generate_dataloader(val_img_dir, \"val\", transform=preprocess_transform_pretrain)\n",
        "\n",
        "# Display batch of validation images\n",
        "show_batch(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54zD7Zy8_nhW"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader_pretrain, lr=0.001, num_epochs=3, log_interval=300, ):\n",
        "\n",
        "  # Move model to designated device (Use GPU when on Colab)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Define hyperparameters and settings\n",
        "  lr = 0.001  # Learning rate\n",
        "  num_epochs = 3  # Number of epochs\n",
        "  log_interval = 300  # Number of iterations before logging\n",
        "\n",
        "  # Set loss function (categorical Cross Entropy Loss)\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Set optimizer (using Adam as default)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  # Setup pytorch-ignite trainer engine\n",
        "  trainer = create_supervised_trainer(model, optimizer, loss_func, device=device)\n",
        "\n",
        "  # Add progress bar to monitor model training\n",
        "  ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"Batch Loss\": x})\n",
        "\n",
        "  # Define evaluation metrics\n",
        "  metrics = {\n",
        "      \"accuracy\": Accuracy(), \n",
        "      \"loss\": Loss(loss_func),\n",
        "  }\n",
        "\n",
        "  # Setup pytorch-ignite evaluator engines. We define two evaluators as they do\n",
        "  # not have exactly similar roles. `evaluator` will save the best model based on \n",
        "  # validation score, whereas `train_evaluator` logs metrics on training set only\n",
        "\n",
        "  # Evaluator for training data\n",
        "  train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
        "\n",
        "  # Evaluator for validation data\n",
        "  evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
        "\n",
        "  # Display message to indicate start of training\n",
        "  @trainer.on(Events.STARTED)\n",
        "  def start_message():\n",
        "      print(\"Begin training\")\n",
        "\n",
        "  # Log results from every batch\n",
        "  @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
        "  def log_batch(trainer):\n",
        "      batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n",
        "      print(\n",
        "          f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n",
        "          f\"Batch {batch} / {trainer.state.epoch_length}: \"\n",
        "          f\"Loss: {trainer.state.output:.3f}\"\n",
        "      )\n",
        "\n",
        "  # Evaluate and print training set metrics\n",
        "  @trainer.on(Events.EPOCH_COMPLETED)\n",
        "  def log_training_loss(trainer):\n",
        "      print(f\"Epoch [{trainer.state.epoch}] - Loss: {trainer.state.output:.2f}\")\n",
        "      train_evaluator.run(train_loader_pretrain)\n",
        "      epoch = trainer.state.epoch\n",
        "      metrics = train_evaluator.state.metrics\n",
        "      print(f\"Train - Loss: {metrics['loss']:.3f}, \"\n",
        "            f\"Accuracy: {metrics['accuracy']:.3f} \"\n",
        "            )\n",
        "\n",
        "  # Evaluate and print validation set metrics\n",
        "  @trainer.on(Events.EPOCH_COMPLETED)\n",
        "  def log_validation_loss(trainer):\n",
        "      evaluator.run(val_loader_pretrain)\n",
        "      epoch = trainer.state.epoch\n",
        "      metrics = evaluator.state.metrics\n",
        "      print(f\"Validation - Loss: {metrics['loss']:.3f}, \"\n",
        "            f\"Accuracy: {metrics['accuracy']:.3f}\"\n",
        "            )\n",
        "      print()\n",
        "      print(\"-\" * 60)\n",
        "      print()\n",
        "\n",
        "  # Sets up checkpoint handler to save best n model(s) based on validation accuracy metric\n",
        "  common.save_best_model_by_val_score(\n",
        "            output_path=\"best_models\",\n",
        "            evaluator=evaluator,\n",
        "            model=model,\n",
        "            metric_name=\"accuracy\",\n",
        "            n_saved=1,\n",
        "            trainer=trainer,\n",
        "            tag=\"val\"\n",
        "  )\n",
        "\n",
        "  # Define a Tensorboard logger\n",
        "  tb_logger = TensorboardLogger(log_dir=\"logs\")\n",
        "\n",
        "  # Using common module to setup tb logger (Alternative method)\n",
        "  # tb_logger = common.setup_tb_logging(\"tb_logs\", trainer, optimizer, evaluators=evaluator)\n",
        "\n",
        "  # Attach handler to plot trainer's loss every n iterations\n",
        "  tb_logger.attach_output_handler(\n",
        "      trainer,\n",
        "      event_name=Events.ITERATION_COMPLETED(every=log_interval),\n",
        "      tag=\"training\",\n",
        "      output_transform=lambda loss: {\"Batch Loss\": loss},\n",
        "  )\n",
        "\n",
        "  # Attach handler to dump evaluator's metrics every epoch completed\n",
        "  for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", evaluator)]:\n",
        "      tb_logger.attach_output_handler(\n",
        "          evaluator,\n",
        "          event_name=Events.EPOCH_COMPLETED,\n",
        "          tag=tag,\n",
        "          metric_names=\"all\",\n",
        "          global_step_transform=global_step_from_engine(trainer),\n",
        "      )\n",
        "\n",
        "\n",
        "  # Start training\n",
        "  trainer.run(train_loader_pretrain, max_epochs=num_epochs)\n",
        "\n",
        "  # Close Tensorboard\n",
        "  tb_logger.close()\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = efficientnet_b3()\n",
        "\n",
        "try:\n",
        "  model.load_state_dict(torch.load('model_weights_tinyimgnet_whole_efficientnetb3.pth'))\n",
        "except:\n",
        "  model = train(model, train_loader_pretrain)\n",
        "  torch.save(model.state_dict(), 'model_weights_tinyimgnet_whole_efficientnetb3.pth')\n",
        "  # files.download('model_weights_tinyimgnet_whole_efficientnetb3.pth') "
      ],
      "metadata": {
        "id": "9LSstoPRcMD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  \n",
        "# Replace the last fully-connected layer\n",
        "model.classifier[1] = nn.Linear(1536, 10)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "KylzQFO2UuRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nTL-86bE_nhX"
      },
      "outputs": [],
      "source": [
        "def get_CIFAR():\n",
        "\n",
        "  transform = T.Compose([T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  batch_size = 64\n",
        "\n",
        "  trainset = CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
        "\n",
        "  testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "  return trainloader, testloader, classes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader, testloader, classes = get_CIFAR()\n",
        "model = train(model, trainloader)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qbB-08L8xC3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxFeJWvAxKpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2fbe1d31051e1135d9cb2da882f47ddb2959ab6493bfdfba0baf475524c40860"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
